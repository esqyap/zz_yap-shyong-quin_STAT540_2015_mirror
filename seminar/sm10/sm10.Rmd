---
title: "sm10"
author: "Eva Y"
date: "March 26, 2015"
output:
  html_document:
    keep_md: yes
---

## Take-home practices:

### Load packages

```{r, message=FALSE, warning=FALSE}
library(MASS)
library(reshape)
library(car)
library(limma)
library(e1071)
library(glmnet)
library(ROCR)
library(CMA)
library(GEOquery)
library(lattice)
library(class)
library(lda)
```

### Data preparation

```{r}
if(file.exists("class_LNstatus.Rdata")) { # if previously downloaded
  load("class_LNstatus.Rdata")
} else { # if downloading for the first time
  # takes a several mins!; returns a list
  datgeo <- getGEO('GSE23177', GSEMatrix = TRUE) 
  dat <- datgeo[[1]]   #Note that dat is an ExpressionSets
  
  str(pData(dat), max.level = 0)
  
  # extract only those variables of interest 
  pData(dat) <- subset(pData(dat),
                       select = c("characteristics_ch1.2",
                                  "characteristics_ch1.3",
                                  "characteristics_ch1"))
  names(pData(dat))<-c("LnStatus", "LnRatio", "Set")

  #Note: LNRatio will not be used in this Seminar. However, you can use it to try some of the regularization techniques learned in class
  
  # split the ExpressionSet into training and test sets. 
  train.es <- dat[, dat$Set == "patient type: training set"]
  test.es <- dat[ , dat$Set != "patient type: training set"]

  #Re-label factor
  pData(train.es)$LnStatus <-
      recode(pData(train.es)$LnStatus, "levels(pData(train.es)$LnStatus)[1]='neg'; else='pos'", levels = c('neg', 'pos'))

  pData(test.es)$LnStatus <-
      recode(pData(test.es)$LnStatus, "levels(pData(test.es)$LnStatus)[1]='neg'; else='pos'",
             levels = c('neg', 'pos'))

  # create data matrices with expression values (probesets in rows). Some of the functions we will use do not take ExpressionSets as objects
  trainDat <- exprs(train.es)
  testDat <- exprs(test.es)

  # Redefine the quantitative variable LnRatio to make it a numeric variable.
  ntrain <- dim(pData(train.es))[1]
  ntest <- dim(pData(test.es))[1]
  
  pData(train.es)$LnRatio <- as.numeric(unlist(strsplit(as.vector(unlist(pData(train.es)$LnRatio)), ":", fixed = TRUE))[(1:ntrain)*2])
  pData(test.es)$LnRatio <- as.numeric(unlist(strsplit(as.vector(unlist(pData(test.es)$LnRatio)), ":", fixed = TRUE))[(1:ntest)*2])

  # save the data to avoid future re-downloading
  save(dat,trainDat,testDat,train.es,test.es, file = "class_LNstatus.Rdata")
}
```

> Exercise 1: perform 100 runs of this CV before selecting a model to test! Add at least one model to the list of models, e.g., use genes with a p-val threshold < cutoff.

Define some parameters.

```{r}
# split samples in training set based on LN status
tabTrain <- table(train.es$LnStatus)

indlist <- sapply(names(tabTrain), function(z) which(train.es$LnStatus == z), simplify = FALSE)

# number of training sets for CV
nfold <- 6

# number of CV runs
nruns <- 100

# use the top-50 limma genes
ngenes <- 50

# number of methods
nmethod <- 8 

# define here an output objects to store results
pr.err <- matrix(-1, nfold*nruns, nmethod, 
                 dimnames=list(paste0("Fold", 
                                      rep(1:nfold, times=nruns)),
                                      c("1NN", "5NN", "10NN", 
                                        "15NN", "LDA",
                                        "Logit", "SVM", "10NN-p")))
```

Modify the loop for feature selection+modeling so that it performs 100 runs of CV. I also added a 10NN model which retains genes at P.Value < 0.05 from the training and test sets. This additional model is named `10NN-p`.

```{r}
for (run in 1:nruns){
  # each row contains 8 pos and 8 negative samples. 
  fold.pos <- matrix(sample(indlist[["pos"]]), nrow=nfold)
  fold.neg <- matrix(sample(indlist[["neg"]]), nrow=nfold)
  
  for(i in 1:nfold){
    # test Fold for the i-th step
    testdat.fold<-trainDat[,c(fold.pos[i,],fold.neg[i,])]
    
    # create a factor of classes for the test set of the i_th fold
    testclass.fold<-train.es$LnStatus[c(fold.pos[i,],fold.neg[i,])]
    
    
    # The rest of the samples are the training set for the i-th step
    traindat.fold<-trainDat[,-c(fold.pos[i,],fold.neg[i,])]
    trainclass.fold<-train.es$LnStatus[-c(fold.pos[i,],fold.neg[i,])]
    
    
    # Step 1: feature selection (do you remember limma?). 
    # Note that a different set of genes will be selected for each fold! you can then compare how consistent these sets were.
    
    limma.dat<-as.data.frame(traindat.fold)
    desMat <- model.matrix(~ trainclass.fold, limma.dat) #design matrix
    trainFit <- lmFit(limma.dat, desMat)
    eBtrainFit <- eBayes(trainFit)
    
    # top-50 limma genes
    top.fold <- topTable(eBtrainFit, coef = which(colnames(coef(trainFit)) != "(Intercept)"),
                       n = ngenes,sort.by="P")
    
    # Retain the top-50 limma genes from the train and test sets
    traindat.fold.top <- traindat.fold[rownames(top.fold),]
    testdat.fold.top <-  testdat.fold[rownames(top.fold),]
    
    # Another method for feature selection
    # use genes with a p-val threshold < 0.05
    top.fold.pval <- topTable(eBtrainFit, coef = which(colnames(coef(trainFit)) != "(Intercept)"), 
                              n = Inf, sort.by="P")
    
    top.fold.pval <- subset(top.fold.pval, P.Value < 0.05)
    
    # Retain genes with adjusted p.value cutoff = 0.05
    traindat.fold.pval <- traindat.fold[rownames(top.fold.pval),]
    testdat.fold.pval <-  testdat.fold[rownames(top.fold.pval),]
    
    
    # STEP 2: select a classifier
    # Set a counter for the method tested
    l <- 0
    
    # kNN classifiers
    for(kk in c(1,5,10,15)) {
      # every time you get inside this loop, the l counter gets redefined (i.e., 1, 2, etc for method 1, method 2, etc)
      l <- l+1
      
      # knn needs samples in rows
      yhat.knn <- knn(train=t(traindat.fold.top), 
                      test=t(testdat.fold.top), cl=trainclass.fold,
                      k = kk)
      
      # Store the prediction error for each kk within this fold
      pr.err[i+((run-1)*nfold),l]<- mean(testclass.fold != yhat.knn)
                          } #end of kNN loop
    
    # LDA method. Note that you can change the prior parameter to reflect a different proportion of case and control samples. The default is to use the class proportions from the training set.
    
    m.lda <- lda(x=t(traindat.fold.top), group=trainclass.fold, prior=c(.5, .5))
    yhat.lda <- predict(m.lda, newdata=t(testdat.fold.top))$class
    pr.err[i+((run-1)*nfold),"LDA"] <-mean(testclass.fold != yhat.lda)
    
    # Logit
    glm.dat <- data.frame(t(traindat.fold.top), group=trainclass.fold)
    
    # 50 factors still will cause optimization warnings  
    # Try without warning suppression to see 
    # To further reduce parameters, regularized regression can be used 
    # To use regularized regression uncomment lines followed by "uncomment for regularized regression" 
    suppressWarnings( m.log <- glm(group ~ ., data=glm.dat,family=binomial) ) 
    
    # uncomment for regularized regression 
    # m.log <- glmnet( t(traindat.fold) , trainclass.fold ,family="binomial") 
    
    pr.log <- predict(m.log,newdata=data.frame(t(testdat.fold.top)),type="response")
    
    # uncomment for regularized regression 
    # pr.log <- predict(m.log,newdata=data.frame(t(testdat.fold)),type="response",newx=t(testdat.fold)) 
    
    pr.cl <- rep(0,length(testclass.fold))
    pr.cl[pr.log > 1/2] <- "pos"
    pr.cl[pr.log <= 1/2] <- "neg"
    
    pr.cl <- factor(pr.cl)
    pr.err[i+((run-1)*nfold),"Logit"] <- mean( pr.cl != testclass.fold )
    
    # SVM
    m.svm <- svm(x=t(traindat.fold.top), y=trainclass.fold, cost=1, type="C-classification", 
               kernel="linear")
    pr.svm <- predict(m.svm,newdata=t(testdat.fold.top)) 
    pr.err[i+((run-1)*nfold),"SVM"] <- mean( pr.svm != testclass.fold)
    
    # 10NN with adjusted p.value cutoff = 0.05
    yhat.knn <- knn(train=t(traindat.fold.pval), 
                      test=t(testdat.fold.pval), cl=trainclass.fold,
                      k = 10)
    pr.err[i+((run-1)*nfold),"10NN-p"]<- mean(testclass.fold != yhat.knn)
    } #end of CV loop
  } # end of 100 runs
```

Get average prediction error for all methods. 

```{r}
cv.err <- colMeans(pr.err)

# mean - 1 sd (sd of the 6 error rates)
ls <- cv.err - apply(pr.err, 2, sd)

# mean + 1 sd (sd of the 6 error rates)
us <- cv.err + apply(pr.err, 2, sd)

# plot the results
plot(1:nmethod, cv.err, ylim=c(0, 1), xlim=c(1, (nmethod+.5)),type='n', 
axes=FALSE, xlab='Classifier', ylab='Error rate',main="6-fold CV Error")

for(j in 1:ncol(pr.err)) 
   points(jitter(rep(j, 600), factor=2), jitter(pr.err[,j]), cex=0.8, pch='X', col='gray')

for(i in 1:nmethod)
   lines(c(i, i), c(ls[i], us[i]), lwd=2, col='black')
points(1:nmethod, ls, pch=19, col='red')
points(1:nmethod, us, pch=19, col='green')
points(1:nmethod, cv.err, pch=19, cex=1.5, col='black')
axis(2, ylab='Error rate')
axis(1, 1:nmethod, colnames(pr.err))

box()
```

The results showed that 15NN seems to perform best. Let's test the 15NN model.

```{r}
yhat.knn <- knn(train=t(trainDat), test=t(testDat), cl=train.es$LnStatus,
                     k = 15)

# store the prediction error for each kk within this fold
pr.errTest<- mean(test.es$LnStatus != yhat.knn)
pr.errTest
```

After performing 100 runs of this CV, 15NN seems to be a better model. When we perform the test on the 15NN model the prediction error is 0.4, which isn't very good but better than the 10NN which was selected previously. 

> Exercise 2: Use AUC as a criteria to select a model based on the training data! Tip: extract the predicted probabilities from each method and use the roc function in ROCR.

```{r}
# make a learningsets object just like what Paul did
m <- matrix(which(dat$Set=="patient type: training set"), 1)

full.learningset <- new("learningsets", learnmatrix=m, method="my own", ntrain=96, iter=1)

fullFeatureScores <- GeneSelection(X=t(exprs(dat)), learningsets= full.learningset, y=dat$LnStatus, method="t.test")
```

Evaluate kNN models. 

```{r}
for (kk in c(1, 5, 10, 15)){
  test.class <- classification(X=t(exprs(dat)), 
                               y=dat$LnStatus,
                               learningsets= full.learningset, 
                               genesel=fullFeatureScores, 
                               nbgene=100, 
                               classifier=pknnCMA, 
                               k=kk)
  print(kk)
  tres <- test.class[[1]]
  ftable(tres)
  roc(tres)
}
```

We can see that the misclassification rate is the lowest for the 15NN model. 
